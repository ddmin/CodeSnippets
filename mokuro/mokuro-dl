#!/usr/bin/python3

import os
import re
import requests
import subprocess

from bs4 import BeautifulSoup
from urllib.parse import unquote


def get_slug(url):
    return re.search(r'\/([^\/]*?)(\/?)$', unquote(url)).group(1)


def create_directory(dirname):
    if not os.path.isdir(f'./{dirname}'):
        try:
            os.mkdir(f'./{dirname}')
        except OSError as error:
            return error


def wget(link, savedir):
    subprocess.run(['wget', '-P', savedir, link])


def main():
    main_url = input("mokura.moe URL:\n> ")

    print(f"\nSending request to {main_url}...")
    raw_html = requests.get(main_url).text

    # remove trailing /
    if main_url[-1] == '/':
        main_url = main_url[:-1]

    soup = BeautifulSoup(raw_html, 'lxml')
    anchors = soup.find_all('a')

    image_directories = []
    mokura_htmls = []
    for a in anchors:
        # image directories
        if re.search(r'(\/$)', a.get('href')) is not None:
            image_directories.append(a)

        # htmls except mobile.html
        if re.search(r'^((?!mobile).)*\.html$', a.get('href')) is not None:
            mokura_htmls.append(a)

    # turn links (Tag) into strings
    image_directories = list(map(lambda x: x.get('href'), image_directories))
    mokura_htmls = list(map(lambda x: x.get('href'), mokura_htmls))

    # remove backlink (first) and _ocr/ (last)
    image_directories = image_directories[1:-1]

    # remove HTML entities and get last part of url
    title = get_slug(main_url)

    # create save directory (./{title})
    create_directory(title)

    # download mokura htmls
    for link in mokura_htmls:
        if re.search(r'^http', link) is None:
            link = f'{main_url}/{link}'
        wget(link, f'./{title}')

    # download galleries
    for directory in image_directories:
        if re.search(r'^http', directory) is None:
            directory = f'{main_url}/{directory}'

        img_dir = get_slug(directory)
        img_save_dir = f'./{title}/{img_dir}'

        # create image save directory
        create_directory(img_save_dir)

        print(f"Sending request to {directory}...")
        img_raw_html = requests.get(directory).text

        img_soup = BeautifulSoup(img_raw_html, 'lxml')
        img_anchors = img_soup.find_all('a')[1:]

        # replace Tag with image string
        img_anchors = list(map(lambda x: x.get('href'), img_anchors))

        # download images
        for img in img_anchors:
            if re.search(r'^http', img) is None:
                img = f'{directory}/{img}'
            wget(img, img_save_dir)


if __name__ == '__main__':
    main()